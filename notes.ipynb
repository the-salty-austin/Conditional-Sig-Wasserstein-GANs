{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.py, the main function of this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "from os import path as pt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from hyperparameters import SIGCWGAN_CONFIGS\n",
    "from lib import ALGOS\n",
    "from lib.algos.base import BaseConfig\n",
    "from lib.data import get_data\n",
    "from lib.plot import savefig, create_summary\n",
    "from lib.utils import pickle_it\n",
    "\n",
    "from torch import nn\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_algo_config(dataset, data_params):\n",
    "    \"\"\" Get the algorithms parameters. \"\"\"\n",
    "    key = dataset\n",
    "    if dataset == 'VAR':\n",
    "        key += str(data_params['dim'])\n",
    "    elif dataset == 'STOCKS':\n",
    "        key += '_' + '_'.join(data_params['assets'])\n",
    "    return SIGCWGAN_CONFIGS[key]\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def get_algo(algo_id, base_config, dataset, data_params, x_real):\n",
    "    if algo_id == 'SigCWGAN':\n",
    "        algo_config = get_algo_config(dataset, data_params)\n",
    "        algo = ALGOS[algo_id](x_real=x_real, config=algo_config, base_config=base_config)\n",
    "    else:\n",
    "        algo = ALGOS[algo_id](x_real=x_real, base_config=base_config)\n",
    "    return algo\n",
    "\n",
    "\n",
    "def get_dataset_configuration(dataset):\n",
    "    if dataset == 'ECG':\n",
    "        generator = [('id=100', dict(filenames=['100']))]\n",
    "    elif dataset == 'STOCKS':\n",
    "        generator = (('_'.join(asset), dict(assets=asset)) for asset in [('SPX',), ('SPX', 'DJI')])\n",
    "    elif dataset == 'BINANCE':\n",
    "        # generator = (('_'.join(asset), dict(assets=asset)) for asset in [('BTC',)])\n",
    "        generator = (('_'.join(asset), dict(assets=asset)) for asset in [('BTC', 'ETH')])\n",
    "        # generator = (('_'.join(asset), dict(assets=asset)) for asset in [('BTC',), ('ETH',), ('BTC', 'ETH')])\n",
    "    elif dataset == 'VAR':\n",
    "        par1 = itertools.product([1], [(0.2, 0.8), (0.5, 0.8), (0.8, 0.8)])\n",
    "        par2 = itertools.product([2], [(0.2, 0.8), (0.5, 0.8), (0.8, 0.8), (0.8, 0.2), (0.8, 0.5)])\n",
    "        par3 = itertools.product([3], [(0.2, 0.8), (0.5, 0.8), (0.8, 0.8), (0.8, 0.2), (0.8, 0.5)])\n",
    "        combinations = itertools.chain(par1, par2, par3)\n",
    "        generator = (\n",
    "            ('dim={}_phi={}_sigma={}'.format(dim, phi, sigma), dict(dim=dim, phi=phi, sigma=sigma))\n",
    "            for dim, (phi, sigma) in combinations\n",
    "        )\n",
    "    elif dataset == 'ARCH':\n",
    "        generator = (('lag={}'.format(lag), dict(lag=lag)) for lag in [3])\n",
    "    elif dataset == 'SINE':\n",
    "        generator = [('a', dict())]\n",
    "    else:\n",
    "        raise Exception('%s not a valid data type.' % dataset)\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the core of `train.py`. Comments are made between the code to explain what each section does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(algo_id, base_config, base_dir, dataset, spec, data_params={}):\n",
    "    \"\"\" Create the experiment directory, calibrate algorithm, store relevant parameters. \"\"\"\n",
    "    print('Executing: %s, %s, %s' % (algo_id, dataset, spec))\n",
    "    experiment_directory = pt.join(base_dir, dataset, spec, 'seed={}'.format(base_config.seed), algo_id)\n",
    "    if not pt.exists(experiment_directory):\n",
    "        # if the experiment directory does not exist we create the directory <<<<\n",
    "        os.makedirs(experiment_directory)\n",
    "    \n",
    "    # >>>> Set seed for exact reproducibility of the experiments <<<<\n",
    "    set_seed(base_config.seed)\n",
    "    \n",
    "    # >>>> initialise dataset and algo <<<<\n",
    "    x_real = get_data(dataset, base_config.p, base_config.q, **data_params)\n",
    "    x_real = x_real.to(base_config.device)\n",
    "\n",
    "    # train test split\n",
    "    # test set is used to compare with data generated by the generator trained by training set.\n",
    "    size_train = int(x_real.shape[0] * 0.8)\n",
    "    indices = np.random.permutation(x_real.shape[0])\n",
    "    train_idx, test_idx = indices[:size_train], indices[size_train:]\n",
    "    x_real_train, x_real_test = x_real[train_idx], x_real[test_idx]\n",
    "\n",
    "    algo = get_algo(algo_id, base_config, dataset, data_params, x_real_train)\n",
    "\n",
    "    # >>>> Train the algorithm <<<<\n",
    "    algo.fit()\n",
    "\n",
    "    # >>> Traing Ends Here <<<\n",
    "\n",
    "    # >>>> create summary <<<<\n",
    "    create_summary(dataset, base_config.device, algo.G, base_config.p, base_config.q, x_real_test, experiment_directory)\n",
    "    savefig('summary.png', experiment_directory)\n",
    "\n",
    "    # >>>> Save generator weights, real path and hyperparameters. <<<<\n",
    "    # >>>> Also, graph the paths to see how different they are. <<<<\n",
    "    pickle_it(x_real, pt.join(pt.dirname(experiment_directory), 'x_real.torch'))\n",
    "    random_indices = torch.randint(0, x_real.shape[0], (250,))\n",
    "    for asset_i in range(x_real.shape[2]):\n",
    "        plt.plot( torch.transpose(x_real[random_indices, base_config.p:, asset_i], 0, 1) , 'C%s' % asset_i, alpha=0.1)\n",
    "    plt.ylim( (-0.2,0.2) )\n",
    "    plt.savefig(os.path.join(experiment_directory, 'x_real.png'))\n",
    "    plt.clf()\n",
    "\n",
    "    pickle_it(x_real_test, pt.join(pt.dirname(experiment_directory), 'x_real_test.torch'))\n",
    "    random_indices = torch.randint(0, x_real_test.shape[0], (250,))\n",
    "    for asset_i in range(x_real_test.shape[2]):\n",
    "        plt.plot( torch.transpose( x_real_test[random_indices, base_config.p:, asset_i], 0, 1) , 'C%s' % asset_i, alpha=0.1)\n",
    "    plt.ylim( (-0.2,0.2) )\n",
    "    plt.savefig(os.path.join(experiment_directory, 'x_real_test.png'))\n",
    "    plt.clf()\n",
    "    \n",
    "    pickle_it(x_real_train, pt.join(pt.dirname(experiment_directory), 'x_real_train.torch'))\n",
    "    random_indices = torch.randint(0, x_real_train.shape[0], (250,))\n",
    "    for asset_i in range(x_real_train.shape[2]):\n",
    "        plt.plot( torch.transpose( x_real_train[random_indices, base_config.p:, asset_i], 0, 1) , 'C%s' % asset_i, alpha=0.1)\n",
    "    plt.ylim( (-0.2,0.2) )\n",
    "    plt.savefig(os.path.join(experiment_directory, 'x_real_train.png'))\n",
    "    plt.clf()\n",
    "\n",
    "    pickle_it(algo.training_loss, pt.join(experiment_directory, 'training_loss.pkl'))\n",
    "    pickle_it(algo.G.to('cpu').state_dict(), pt.join(experiment_directory, 'G_weights.torch'))\n",
    "    \n",
    "    # >>>> Log some results <<<<\n",
    "    algo.plot_losses()\n",
    "    savefig('losses', experiment_directory)\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    if not pt.exists('./data'):\n",
    "        os.mkdir('./data')\n",
    "\n",
    "    print('Start of training. CUDA: %s' % args.use_cuda)\n",
    "    for dataset in args.datasets:\n",
    "        for algo_id in args.algos:\n",
    "            for seed in range(args.initial_seed, args.initial_seed + args.num_seeds):\n",
    "                \n",
    "                print(f\"dataset={dataset} / algo={algo_id} / seed={seed}\")\n",
    "                \n",
    "                base_config = BaseConfig(\n",
    "                        device='cuda:{}'.format(args.device) if args.use_cuda and torch.cuda.is_available() else 'cpu',\n",
    "                    seed=seed,\n",
    "                    batch_size=args.batch_size,\n",
    "                    hidden_dims=args.hidden_dims,\n",
    "                    p=args.p,\n",
    "                    q=args.q,\n",
    "                    total_steps=args.total_steps,\n",
    "                    mc_samples=1000,\n",
    "                )\n",
    "                set_seed(seed)\n",
    "                generator = get_dataset_configuration(dataset)\n",
    "                for spec, data_params in generator:\n",
    "                    run(\n",
    "                        algo_id=algo_id,\n",
    "                        base_config=base_config,\n",
    "                        data_params=data_params,\n",
    "                        dataset=dataset,\n",
    "                        base_dir=args.base_dir,\n",
    "                        spec=spec,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start training, run the block below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "class Args(argparse.Namespace):\n",
    "    base_dir     = './numerical_results'\n",
    "    use_cuda     = 'store_true'\n",
    "    device       = 0\n",
    "    num_seeds    = 1\n",
    "    initial_seed = 0\n",
    "    datasets     = ['BINANCE', ]\n",
    "    algos        = ['CWGAN','SigCWGAN',]\n",
    "    batch_size   = 200\n",
    "    p            = 24\n",
    "    q            =  6\n",
    "    hidden_dims  = 3 * (50,)\n",
    "    total_steps  = 100\n",
    "\n",
    "args = Args()\n",
    "main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notations\n",
    "\n",
    "* $N$ is the total number of closing prices for each asset.\n",
    "* $d$ is the number of total assets.\n",
    "* $p$ is the length of past data that we are conditioning on.\n",
    "* $q$ is the length of generated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `get_data()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, call `get_binance_dataset()` which reads the csv of each Binance asset and concatenates all of their closing prices into a 3D-tensor whose size is $(1,N,d)$. Here, $N$ is the total number of closing prices for each asset and $d$ is the number of total assets.\n",
    "\n",
    "Then, the $(1,N,d)$ tensor is thrown into `zero_based_rolling_window()`. The output is a $( \\, N-(p+q) \\, , \\, p+q \\, , \\, d \\, )$ 3D-tensor, where $p$ is the length of past data that we are conditioning on and $q$ is the length of generated data. This is done by following this procedure:\n",
    "\n",
    "1. Call each entry of the second dimension of the $(1,N,d)$ tensor $x_t$, so $t=0,1,\\ldots,N-1$.\n",
    "2. For $t=0$ to $N-(p+q)$ (the start of each windonw)\n",
    "    1. First take the next $p+q$ $x_t$'s, which are $x_t,\\ldots,x_{t+(p+q-1)}$.\n",
    "    2. Compute $y_{s} := \\dfrac{ (x_s-x_t) }{ x_t }$ for $s=t,t+1,\\ldots,t+(p+q-1)$, which is the relative change of price to the price at the start of the window.\n",
    "    3. Collect $y_t,y_{t+1},\\ldots,y_{t+(p+q-1)}$ to form a $(1,p+q,d)$ tensor.\n",
    "3. Collect all $(1,p+q,d)$ tensors to form a $(N-(p+q),p+q,d)$ tensor. Return this tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `algo.fit()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `GANs.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `GAN` whose base class is `BaseAlgo` is equipped with a `ResFNN` discriminator and a `SimpleGenerator` generator from `base.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SimpleGenerator` has a `ArFNN` (autoregressive feedforward neural network) architecture and works as follows:\n",
    "\n",
    "1. `input_dim` $= p \\times d$, `output_dim` $= d$, `hidden_dims` $=(50,50,50)$, and `latent_dim` $= d$ are the inputs to initialize `SimpleGenerator`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Architecture of AR-FNN*\n",
    "\n",
    "$$ (x,z) \\in \\mathbb{R}^{p \\times d} \\times \\mathbb{R}^{1 \\times d} = \\mathbb{R}^{(p+1) \\times d} \\overset{A_1}\\longrightarrow \\mathbb{R}^{50} \\overset{\\phi_\\alpha}\\longrightarrow \\mathbb{R}^{50} \\overset{R_2}\\longrightarrow \\mathbb{R}^{50} \\overset{R_3}\\longrightarrow \\mathbb{R}^{50} \\overset{A_4}\\longrightarrow \\mathbb{R}^{d} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `gans.py`, the generator `G` calls the `sample()` function, which iteratively generates the future path according to Algorithm 1 on Page 15. See the comments in code to understand how input data is transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArFNN(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dims: Tuple[int]):\n",
    "        pass\n",
    "\n",
    "    def forward(self, z, x_past):\n",
    "        x_generated = list()\n",
    "        for t in range(z.shape[1]):\n",
    "            # d=2, p=24\n",
    "            # Layer A_1 starts here\n",
    "            z_t = z[:, t:t + 1]\n",
    "            # z_t: torch.Size([200000, 1, d=2])  x:torch.Size([200000, 1, p*d=48])\n",
    "            x_in = torch.cat([z_t, x_past.reshape(x_past.shape[0], 1, -1)], dim=-1)\n",
    "            # x_in: torch.Size([200000, 1, d*(p+1)=50])\n",
    "            # Layer A_1 outputs here\n",
    "            \n",
    "            # >>> ResFNN Generator <<<\n",
    "            x_gen = self.network(x_in)  # Calls ResFNN().network(), layer A_4 outputs here.\n",
    "            # x_gen:torch.Size([200000, 1, d=2])\n",
    "            \n",
    "            x_past = torch.cat([x_past[:, 1:], x_gen], dim=1) # iterative replace and append\n",
    "            # x_past:torch.Size([200000, 1, p=24])\n",
    "            x_generated.append(x_gen)\n",
    "        x_fake = torch.cat(x_generated, dim=1)\n",
    "        return x_fake\n",
    "\n",
    "class SimpleGenerator(ArFNN):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dims: Tuple[int], latent_dim: int):\n",
    "        super(SimpleGenerator, self).__init__(input_dim + latent_dim, output_dim, hidden_dims)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def sample(self, steps, x_past):\n",
    "        '''\n",
    "        [Usage] generator.sample( q, x_past ) where x_past has length p.\n",
    "        '''\n",
    "        # self.latent_dim = d\n",
    "        z = torch.randn(x_past.size(0), steps, self.latent_dim).to(x_past.device)\n",
    "        return self.forward(z, x_past)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every loop in `ArFNN.forward()`, the network of `ResFNN` is instantiated. In our example, `hiddem_dims` took $(50,50,50)$ as an input, so, as a result of the for loop below, there will be two $R_i$'s, outputing a $\\mathbb{R}^{50}$ tensor after $R_3$. Finally, one more `Linear` layer ($A_4$) is appended. This maps  $\\mathbb{R}^{50}$ to $\\mathbb{R}^d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 24\n",
    "d = 2\n",
    "\n",
    "class ResidualBlock():\n",
    "    def __init__(self, input_dim: int, output_dim: int) -> None:\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.activation = nn.PReLU()\n",
    "        self.create_residual_connection = True if input_dim == output_dim else False\n",
    "\n",
    "class ResFNN(nn.Module):\n",
    "    def __init__(self, input_dim=(p+1)*d, output_dim=d, hidden_dims=(50,50,50), flatten: bool = False):\n",
    "        blocks = list()\n",
    "        input_dim_block = input_dim  # initially R^{ (p+1) * d }\n",
    "        for hidden_dim in hidden_dims:\n",
    "            blocks.append(ResidualBlock(input_dim_block, hidden_dim))  # blocks A_1, R_2, and R_3\n",
    "            input_dim_block = hidden_dim  # becomes R^{ 50 }\n",
    "        blocks.append(nn.Linear(input_dim_block, output_dim))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
