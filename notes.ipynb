{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.py, the main function of this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "from os import path as pt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from hyperparameters import SIGCWGAN_CONFIGS\n",
    "from lib import ALGOS\n",
    "from lib.algos.base import BaseConfig\n",
    "from lib.data import get_data\n",
    "from lib.plot import savefig, create_summary\n",
    "from lib.utils import pickle_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_algo_config(dataset, data_params):\n",
    "    \"\"\" Get the algorithms parameters. \"\"\"\n",
    "    key = dataset\n",
    "    if dataset == 'VAR':\n",
    "        key += str(data_params['dim'])\n",
    "    elif dataset == 'STOCKS':\n",
    "        key += '_' + '_'.join(data_params['assets'])\n",
    "    return SIGCWGAN_CONFIGS[key]\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def get_algo(algo_id, base_config, dataset, data_params, x_real):\n",
    "    if algo_id == 'SigCWGAN':\n",
    "        algo_config = get_algo_config(dataset, data_params)\n",
    "        algo = ALGOS[algo_id](x_real=x_real, config=algo_config, base_config=base_config)\n",
    "    else:\n",
    "        algo = ALGOS[algo_id](x_real=x_real, base_config=base_config)\n",
    "    return algo\n",
    "\n",
    "\n",
    "def get_dataset_configuration(dataset):\n",
    "    if dataset == 'ECG':\n",
    "        generator = [('id=100', dict(filenames=['100']))]\n",
    "    elif dataset == 'STOCKS':\n",
    "        generator = (('_'.join(asset), dict(assets=asset)) for asset in [('SPX',), ('SPX', 'DJI')])\n",
    "    elif dataset == 'BINANCE':\n",
    "        # generator = (('_'.join(asset), dict(assets=asset)) for asset in [('BTC',)])\n",
    "        generator = (('_'.join(asset), dict(assets=asset)) for asset in [('BTC', 'ETH')])\n",
    "        # generator = (('_'.join(asset), dict(assets=asset)) for asset in [('BTC',), ('ETH',), ('BTC', 'ETH')])\n",
    "    elif dataset == 'VAR':\n",
    "        par1 = itertools.product([1], [(0.2, 0.8), (0.5, 0.8), (0.8, 0.8)])\n",
    "        par2 = itertools.product([2], [(0.2, 0.8), (0.5, 0.8), (0.8, 0.8), (0.8, 0.2), (0.8, 0.5)])\n",
    "        par3 = itertools.product([3], [(0.2, 0.8), (0.5, 0.8), (0.8, 0.8), (0.8, 0.2), (0.8, 0.5)])\n",
    "        combinations = itertools.chain(par1, par2, par3)\n",
    "        generator = (\n",
    "            ('dim={}_phi={}_sigma={}'.format(dim, phi, sigma), dict(dim=dim, phi=phi, sigma=sigma))\n",
    "            for dim, (phi, sigma) in combinations\n",
    "        )\n",
    "    elif dataset == 'ARCH':\n",
    "        generator = (('lag={}'.format(lag), dict(lag=lag)) for lag in [3])\n",
    "    elif dataset == 'SINE':\n",
    "        generator = [('a', dict())]\n",
    "    else:\n",
    "        raise Exception('%s not a valid data type.' % dataset)\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the core of `train.py`. Comments are made between the code to explain what each section does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(algo_id, base_config, base_dir, dataset, spec, data_params={}):\n",
    "    \"\"\" Create the experiment directory, calibrate algorithm, store relevant parameters. \"\"\"\n",
    "    print('Executing: %s, %s, %s' % (algo_id, dataset, spec))\n",
    "    experiment_directory = pt.join(base_dir, dataset, spec, 'seed={}'.format(base_config.seed), algo_id)\n",
    "    if not pt.exists(experiment_directory):\n",
    "        # if the experiment directory does not exist we create the directory <<<<\n",
    "        os.makedirs(experiment_directory)\n",
    "    \n",
    "    # >>>> Set seed for exact reproducibility of the experiments <<<<\n",
    "    set_seed(base_config.seed)\n",
    "    \n",
    "    # >>>> initialise dataset and algo <<<<\n",
    "    x_real = get_data(dataset, base_config.p, base_config.q, **data_params)\n",
    "    x_real = x_real.to(base_config.device)\n",
    "\n",
    "    # train test split\n",
    "    # test set is used to compare with data generated by the generator trained by training set.\n",
    "    size_train = int(x_real.shape[0] * 0.8)\n",
    "    indices = np.random.permutation(x_real.shape[0])\n",
    "    train_idx, test_idx = indices[:size_train], indices[size_train:]\n",
    "    x_real_train, x_real_test = x_real[train_idx], x_real[test_idx]\n",
    "\n",
    "    algo = get_algo(algo_id, base_config, dataset, data_params, x_real_train)\n",
    "\n",
    "    # >>>> Train the algorithm <<<<\n",
    "    algo.fit()\n",
    "\n",
    "    # >>> Traing Ends Here <<<\n",
    "\n",
    "    # >>>> create summary <<<<\n",
    "    create_summary(dataset, base_config.device, algo.G, base_config.p, base_config.q, x_real_test, experiment_directory)\n",
    "    savefig('summary.png', experiment_directory)\n",
    "\n",
    "    # >>>> Save generator weights, real path and hyperparameters. <<<<\n",
    "    # >>>> Also, graph the paths to see how different they are. <<<<\n",
    "    pickle_it(x_real, pt.join(pt.dirname(experiment_directory), 'x_real.torch'))\n",
    "    random_indices = torch.randint(0, x_real.shape[0], (250,))\n",
    "    for asset_i in range(x_real.shape[2]):\n",
    "        plt.plot( torch.transpose(x_real[random_indices, base_config.p:, asset_i], 0, 1) , 'C%s' % asset_i, alpha=0.1)\n",
    "    plt.ylim( (-0.2,0.2) )\n",
    "    plt.savefig(os.path.join(experiment_directory, 'x_real.png'))\n",
    "    plt.clf()\n",
    "\n",
    "    pickle_it(x_real_test, pt.join(pt.dirname(experiment_directory), 'x_real_test.torch'))\n",
    "    random_indices = torch.randint(0, x_real_test.shape[0], (250,))\n",
    "    for asset_i in range(x_real_test.shape[2]):\n",
    "        plt.plot( torch.transpose( x_real_test[random_indices, base_config.p:, asset_i], 0, 1) , 'C%s' % asset_i, alpha=0.1)\n",
    "    plt.ylim( (-0.2,0.2) )\n",
    "    plt.savefig(os.path.join(experiment_directory, 'x_real_test.png'))\n",
    "    plt.clf()\n",
    "    \n",
    "    pickle_it(x_real_train, pt.join(pt.dirname(experiment_directory), 'x_real_train.torch'))\n",
    "    random_indices = torch.randint(0, x_real_train.shape[0], (250,))\n",
    "    for asset_i in range(x_real_train.shape[2]):\n",
    "        plt.plot( torch.transpose( x_real_train[random_indices, base_config.p:, asset_i], 0, 1) , 'C%s' % asset_i, alpha=0.1)\n",
    "    plt.ylim( (-0.2,0.2) )\n",
    "    plt.savefig(os.path.join(experiment_directory, 'x_real_train.png'))\n",
    "    plt.clf()\n",
    "\n",
    "    pickle_it(algo.training_loss, pt.join(experiment_directory, 'training_loss.pkl'))\n",
    "    pickle_it(algo.G.to('cpu').state_dict(), pt.join(experiment_directory, 'G_weights.torch'))\n",
    "    \n",
    "    # >>>> Log some results <<<<\n",
    "    algo.plot_losses()\n",
    "    savefig('losses', experiment_directory)\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    if not pt.exists('./data'):\n",
    "        os.mkdir('./data')\n",
    "\n",
    "    print('Start of training. CUDA: %s' % args.use_cuda)\n",
    "    for dataset in args.datasets:\n",
    "        for algo_id in args.algos:\n",
    "            for seed in range(args.initial_seed, args.initial_seed + args.num_seeds):\n",
    "                \n",
    "                print(f\"dataset={dataset} / algo={algo_id} / seed={seed}\")\n",
    "                \n",
    "                base_config = BaseConfig(\n",
    "                        device='cuda:{}'.format(args.device) if args.use_cuda and torch.cuda.is_available() else 'cpu',\n",
    "                    seed=seed,\n",
    "                    batch_size=args.batch_size,\n",
    "                    hidden_dims=args.hidden_dims,\n",
    "                    p=args.p,\n",
    "                    q=args.q,\n",
    "                    total_steps=args.total_steps,\n",
    "                    mc_samples=1000,\n",
    "                )\n",
    "                set_seed(seed)\n",
    "                generator = get_dataset_configuration(dataset)\n",
    "                for spec, data_params in generator:\n",
    "                    run(\n",
    "                        algo_id=algo_id,\n",
    "                        base_config=base_config,\n",
    "                        data_params=data_params,\n",
    "                        dataset=dataset,\n",
    "                        base_dir=args.base_dir,\n",
    "                        spec=spec,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start training, run the block below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "class Args(argparse.Namespace):\n",
    "    base_dir     = './numerical_results'\n",
    "    use_cuda     = 'store_true'\n",
    "    device       = 0\n",
    "    num_seeds    = 1\n",
    "    initial_seed = 0\n",
    "    datasets     = ['BINANCE', ]\n",
    "    algos        = ['CWGAN','SigCWGAN',]\n",
    "    batch_size   = 200\n",
    "    p            = 24\n",
    "    q            =  6\n",
    "    hidden_dims  = 3 * (50,)\n",
    "    total_steps  = 100\n",
    "\n",
    "args = Args()\n",
    "main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `get_data()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, call `get_binance_dataset()` which reads the csv of each Binance asset and concatenates all of their closing prices into a 3D-tensor whose size is $(1,N,d)$. Here, $N$ is the total number of closing prices for each asset and $d$ is the number of total assets.\n",
    "\n",
    "Then, the $(1,N,d)$ tensor is thrown into `zero_based_rolling_window()`. The output is a $( \\, N-(p+q) \\, , \\, p+q \\, , \\, d \\, )$ 3D-tensor, where $p$ is the length of past data that we are conditioning on and $q$ is the length of generated data. This is done by following this procedure:\n",
    "\n",
    "1. Call each entry of the second dimension of the $(1,N,d)$ tensor $\\bold{x}_t$, so $t=0,1,\\ldots,N-1$.\n",
    "2. First take the first $p+q$ $\\bold{x}_t$'s, which are $\\bold{x}_0,\\ldots,\\bold{x}_{p+q-1}$.\n",
    "3. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
