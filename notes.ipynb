{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.py, the main function of this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "from os import path as pt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from hyperparameters import SIGCWGAN_CONFIGS\n",
    "from lib import ALGOS\n",
    "from lib.algos.base import BaseConfig\n",
    "from lib.data import get_data\n",
    "from lib.plot import savefig, create_summary\n",
    "from lib.utils import pickle_it\n",
    "\n",
    "from train import *\n",
    "\n",
    "from torch import nn\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the core of `train.py`. Comments are made between the code to explain what each section does.\n",
    "\n",
    "The main parts are:\n",
    "\n",
    "1. `get_data()`\n",
    "2. `algo.fit()`\n",
    "3. Plot & Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(algo_id, base_config, base_dir, dataset, spec, data_params={}):\n",
    "    \"\"\" Create the experiment directory, calibrate algorithm, store relevant parameters. \"\"\"\n",
    "    print('Executing: %s, %s, %s' % (algo_id, dataset, spec))\n",
    "    experiment_directory = pt.join(base_dir, dataset, spec, 'seed={}'.format(base_config.seed), algo_id)\n",
    "    if not pt.exists(experiment_directory):\n",
    "        # if the experiment directory does not exist we create the directory <<<<\n",
    "        os.makedirs(experiment_directory)\n",
    "    \n",
    "    # >>>> Set seed for exact reproducibility of the experiments <<<<\n",
    "    set_seed(base_config.seed)\n",
    "    \n",
    "    # >>>> initialise dataset and algo <<<<\n",
    "    x_real = get_data(dataset, base_config.p, base_config.q, **data_params)\n",
    "    x_real = x_real.to(base_config.device)\n",
    "\n",
    "    # train test split\n",
    "    # test set is used to compare with data generated by the generator trained by training set.\n",
    "    size_train = int(x_real.shape[0] * 0.8)\n",
    "    indices = np.random.permutation(x_real.shape[0])\n",
    "    train_idx, test_idx = indices[:size_train], indices[size_train:]\n",
    "    x_real_train, x_real_test = x_real[train_idx], x_real[test_idx]\n",
    "\n",
    "    algo = get_algo(algo_id, base_config, dataset, data_params, x_real_train)\n",
    "\n",
    "    # >>>> Train the algorithm <<<<\n",
    "    algo.fit()\n",
    "\n",
    "    # >>> Traing Ends Here <<<\n",
    "\n",
    "    # >>>> create summary <<<<\n",
    "    create_summary(dataset, base_config.device, algo.G, base_config.p, base_config.q, x_real_test, experiment_directory)\n",
    "    savefig('summary.png', experiment_directory)\n",
    "\n",
    "    # >>>> Save generator weights, real path and hyperparameters. <<<<\n",
    "    # >>>> Also, graph the paths to see how different they are. <<<<\n",
    "    pickle_it(x_real, pt.join(pt.dirname(experiment_directory), 'x_real.torch'))\n",
    "    random_indices = torch.randint(0, x_real.shape[0], (250,))\n",
    "    for asset_i in range(x_real.shape[2]):\n",
    "        plt.plot( torch.transpose(x_real[random_indices, base_config.p:, asset_i], 0, 1) , 'C%s' % asset_i, alpha=0.1)\n",
    "    plt.ylim( (-0.2,0.2) )\n",
    "    plt.savefig(os.path.join(experiment_directory, 'x_real.png'))\n",
    "    plt.clf()\n",
    "\n",
    "    pickle_it(x_real_test, pt.join(pt.dirname(experiment_directory), 'x_real_test.torch'))\n",
    "    random_indices = torch.randint(0, x_real_test.shape[0], (250,))\n",
    "    for asset_i in range(x_real_test.shape[2]):\n",
    "        plt.plot( torch.transpose( x_real_test[random_indices, base_config.p:, asset_i], 0, 1) , 'C%s' % asset_i, alpha=0.1)\n",
    "    plt.ylim( (-0.2,0.2) )\n",
    "    plt.savefig(os.path.join(experiment_directory, 'x_real_test.png'))\n",
    "    plt.clf()\n",
    "    \n",
    "    pickle_it(x_real_train, pt.join(pt.dirname(experiment_directory), 'x_real_train.torch'))\n",
    "    random_indices = torch.randint(0, x_real_train.shape[0], (250,))\n",
    "    for asset_i in range(x_real_train.shape[2]):\n",
    "        plt.plot( torch.transpose( x_real_train[random_indices, base_config.p:, asset_i], 0, 1) , 'C%s' % asset_i, alpha=0.1)\n",
    "    plt.ylim( (-0.2,0.2) )\n",
    "    plt.savefig(os.path.join(experiment_directory, 'x_real_train.png'))\n",
    "    plt.clf()\n",
    "\n",
    "    pickle_it(algo.training_loss, pt.join(experiment_directory, 'training_loss.pkl'))\n",
    "    pickle_it(algo.G.to('cpu').state_dict(), pt.join(experiment_directory, 'G_weights.torch'))\n",
    "    \n",
    "    # >>>> Log some results <<<<\n",
    "    algo.plot_losses()\n",
    "    savefig('losses', experiment_directory)\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    if not pt.exists('./data'):\n",
    "        os.mkdir('./data')\n",
    "\n",
    "    print('Start of training. CUDA: %s' % args.use_cuda)\n",
    "    for dataset in args.datasets:\n",
    "        for algo_id in args.algos:\n",
    "            for seed in range(args.initial_seed, args.initial_seed + args.num_seeds):\n",
    "                \n",
    "                print(f\"dataset={dataset} / algo={algo_id} / seed={seed}\")\n",
    "                \n",
    "                base_config = BaseConfig(\n",
    "                        device='cuda:{}'.format(args.device) if args.use_cuda and torch.cuda.is_available() else 'cpu',\n",
    "                    seed=seed,\n",
    "                    batch_size=args.batch_size,\n",
    "                    hidden_dims=args.hidden_dims,\n",
    "                    p=args.p,\n",
    "                    q=args.q,\n",
    "                    total_steps=args.total_steps,\n",
    "                    mc_samples=1000,\n",
    "                )\n",
    "                set_seed(seed)\n",
    "                generator = get_dataset_configuration(dataset)\n",
    "                for spec, data_params in generator:\n",
    "                    run(\n",
    "                        algo_id=algo_id,\n",
    "                        base_config=base_config,\n",
    "                        data_params=data_params,\n",
    "                        dataset=dataset,\n",
    "                        base_dir=args.base_dir,\n",
    "                        spec=spec,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To start training, run the block below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "class Args(argparse.Namespace):\n",
    "    base_dir     = './numerical_results'\n",
    "    use_cuda     = 'store_true'\n",
    "    device       = 0\n",
    "    num_seeds    = 1\n",
    "    initial_seed = 0\n",
    "    datasets     = ['BINANCE', ]\n",
    "    algos        = ['CWGAN','SigCWGAN',]\n",
    "    batch_size   = 200\n",
    "    p            = 24\n",
    "    q            = 6\n",
    "    hidden_dims  = 3 * (50,)\n",
    "    total_steps  = 100\n",
    "\n",
    "args = Args()\n",
    "main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notations\n",
    "\n",
    "* $N$ is the total number of closing prices for each asset.\n",
    "* $d$ is the number of total assets.\n",
    "* $p$ is the length of past data that we are conditioning on.\n",
    "* $q$ is the length of generated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1] `get_data()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, call `get_binance_dataset()` which reads the csv of each Binance asset and concatenates all of their closing prices into a 3D-tensor whose size is $(1,N,d)$. Here, $N$ is the total number of closing prices for each asset and $d$ is the number of total assets.\n",
    "\n",
    "Then, the $(1,N,d)$ tensor is thrown into `zero_based_rolling_window()`. The output is a $( \\, N-(p+q) \\, , \\, p+q \\, , \\, d \\, )$ 3D-tensor, where $p$ is the length of past data that we are conditioning on and $q$ is the length of generated data. This is done by following this procedure:\n",
    "\n",
    "1. Call each entry of the second dimension of the $(1,N,d)$ tensor $x_t$, so $t=0,1,\\ldots,N-1$.\n",
    "2. For $t=0$ to $N-(p+q)$ (the start of each windonw)\n",
    "    1. First take the next $p+q$ $x_t$'s, which are $x_t,\\ldots,x_{t+(p+q-1)}$.\n",
    "    2. Compute $y_{s} := \\dfrac{ (x_s-x_t) }{ x_t }$ for $s=t,t+1,\\ldots,t+(p+q-1)$, which is the relative change of price to the price at the start of the window.\n",
    "    3. Collect $y_t,y_{t+1},\\ldots,y_{t+(p+q-1)}$ to form a $(1,p+q,d)$ tensor.\n",
    "3. Collect all $(1,p+q,d)$ tensors to form a $(N-(p+q),p+q,d)$ tensor. Return this tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2] `algo.fit()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit()` function is defined by the `BaseAlgo` class. When the `fit()` function is called, we enter a training loop whose number of iterations is determined by `base_config.total_steps`. This value decides how many times the generator is trained before stopping.\n",
    "\n",
    "An iteration is called a `step()`, which is defined by `gans.py`, `gmmn.py`, `sigcwgan.py` depending on the algorithm. I will explain how `step()` works under different algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `GANs.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `GAN` whose base class is `BaseAlgo` is equipped with:\n",
    "* a `ResFNN` discriminator ( `D` ) from `./lib/arfnn.py`\n",
    "* a `SimpleGenerator` generator ( `G` ) from `./lib/algos/base.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pseudocode for `step()` is as follows:\n",
    "\n",
    "1. Loop `self.D_steps_per_G_step` times (Train the discriminator this many times.)\n",
    "    1. Randomly sample some real paths of length $p+q$. The number of real paths is decided by `self.batch_size`.\n",
    "    2. Regard the generator as given, for eaxh real path, use the first $p$ entries to generate the next $q$ entries (fake path of length $q$, conditional on historic data of length $p$). This is done by calling `G.sample().`\n",
    "    3. Concatenate the real part and fake part to form length-$(p+q)$ paths.\n",
    "    4. Train the discriminator by comparing how different [the entirely-real paths] and [the paths with fake parts] are. This is done by calling `D_trainstep()`.\n",
    "    5. Record the loss from the discriminator.\n",
    "2. Randomly sample some length-$p$ real paths and generate length-$q$ future paths.\n",
    "3. Concatenate them into a length-$(p+q)$ paths.\n",
    "4. Regard the discriminator as given, train the generator by using the discriminator to compare how different [the entirely-real paths] and [the paths with fake parts] are. This is done by calling `G_trainstep()`.\n",
    "5. Record the loss from the generator.\n",
    "\n",
    "Notice that I mention \"comparison between real and fake data\" in the pseudocode. The pseudocode is as follows:\n",
    "1. Provide the discriminator with entirely-real data.\n",
    "2. Measure the loss between the real data and $1$.\n",
    "3. Provide the discriminator with data with fake parts.\n",
    "4. Measure the loss between the fake data and $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ResFNN` discriminator has inputs:\n",
    "* `input_dim` $= (p+q) \\times d$,\n",
    "* `hidden_dims` $= (50,50,50)$,\n",
    "* `output_dim` $= 1$ which is a $[0,1]$ output. If the value is close to $1$, the discriminator thinks the input is real. If it's close to $0$, its considered fake/generated.\n",
    "\n",
    "There are four GANs to try with:\n",
    "1. Recurrent Conditional GAN (`RCGAN`)\n",
    "2. Time-Series GAN (`TimeGAN`)\n",
    "\n",
    "The loss function of 1. & 2. is `torch.nn.functional.binary_cross_entropy_with_logits()`\n",
    "\n",
    "3. Recurrent Conditional Weierstrass GAN (`RCWGAN`)\n",
    "4. ConditionalWGAN (`CWGAN`)\n",
    "\n",
    "The loss function of 3. & 4. is\n",
    "$$ (2 \\times \\text{target} - 1) \\times \\text{discriminator}_{\\text{out}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SimpleGenerator` has a `ArFNN` (autoregressive feedforward neural network) architecture:\n",
    "\n",
    "$$ (x,z) \\in \\mathbb{R}^{p \\times d} \\times \\mathbb{R}^{1 \\times d} = \\mathbb{R}^{(p+1) \\times d} \\overset{A_1}\\longrightarrow \\mathbb{R}^{50} \\overset{\\phi_\\alpha}\\longrightarrow \\mathbb{R}^{50} \\overset{R_2}\\longrightarrow \\mathbb{R}^{50} \\overset{R_3}\\longrightarrow \\mathbb{R}^{50} \\overset{A_4}\\longrightarrow \\mathbb{R}^{d} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `gans.py`, the generator `G` calls the `sample()` function, which iteratively generates the future path according to Algorithm 1 on Page 15. \n",
    "\n",
    "In short, Algorithm 1 uses past data of length $p$ to generate one new value, then uses the past $p-1$ data plus the newly-generated data (so there are still $p$ data in total) to generate one extra new data. This procedure is done iteratively until we have generated and collected a path of length $q$. \n",
    "\n",
    "See the comments in code to understand how input data is transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResFNN(nn.Module):\n",
    "    pass  # omitted\n",
    "\n",
    "class ArFNN(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dims: Tuple[int]):\n",
    "        super().__init__()\n",
    "        self.network = ResFNN(input_dim, output_dim, hidden_dims)\n",
    "\n",
    "    def forward(self, z, x_past):\n",
    "        x_generated = list()\n",
    "        for t in range(z.shape[1]):\n",
    "            # d=2, p=24\n",
    "            z_t = z[:, t:t+1]\n",
    "            # z_t: torch.Size([200000, 1, d=2])  x:torch.Size([200000, 1, p*d=48])\n",
    "            x_in = torch.cat([z_t, x_past.reshape(x_past.shape[0], 1, -1)], dim=-1)\n",
    "            # x_in: torch.Size([200000, 1, d*(p+1)=50])\n",
    "            # (x,z) is created as x_in\n",
    "            \n",
    "            # >>> ResFNN Generator <<<\n",
    "            x_gen = self.network(x_in)  # Calls ResFNN().network(), layer A_4 outputs here. See below for more.\n",
    "            # x_gen:torch.Size([200000, 1, d=2])\n",
    "            \n",
    "            x_past = torch.cat([x_past[:, 1:], x_gen], dim=1) # iterative replace and append\n",
    "            # x_past:torch.Size([200000, 1, p=24])\n",
    "            x_generated.append(x_gen)\n",
    "        x_fake = torch.cat(x_generated, dim=1)\n",
    "        return x_fake\n",
    "\n",
    "class SimpleGenerator(ArFNN):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dims: Tuple[int], latent_dim: int):\n",
    "        super(SimpleGenerator, self).__init__(input_dim + latent_dim, output_dim, hidden_dims)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def sample(self, steps, x_past):\n",
    "        '''\n",
    "        [Usage] generator.sample( q, x_past ) where x_past has length p.\n",
    "        '''\n",
    "        # self.latent_dim = d\n",
    "        z = torch.randn(x_past.size(0), steps, self.latent_dim).to(x_past.device)\n",
    "        return self.forward(z, x_past)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that up until `network()` is called, we have a $(x,z) \\in \\mathbb{R}^{(p+1) \\times d}$ tensor.\n",
    "\n",
    "When `network()` is called, the network of `ResFNN` is instantiated, which takes the $\\mathbb{R}^{(p+1) \\times d}$ tensor as input. We set `hiddem_dims` to be $(50,50,50)$, so, as a result of the for loop below, there will be three `ResidualBlocks` ($\\phi_\\alpha \\circ A_1: \\mathbb{R}^{(p+1) \\times d} \\rightarrow \\mathbb{R}^{50}$, $R_2: \\mathbb{R}^{50} \\rightarrow \\mathbb{R}^{50}$, and $R_3: \\mathbb{R}^{50} \\rightarrow \\mathbb{R}^{50}$). Finally, one more `Linear` layer ($A_4$) is appended, which maps  $\\mathbb{R}^{50}$ to $\\mathbb{R}^d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 24\n",
    "d = 2\n",
    "\n",
    "class ResidualBlock():\n",
    "    def __init__(self, input_dim: int, output_dim: int) -> None:\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.activation = nn.PReLU()\n",
    "        self.create_residual_connection = True if input_dim == output_dim else False\n",
    "\n",
    "class ResFNN(nn.Module):\n",
    "    def __init__(self, input_dim=(p+1)*d, output_dim=d, hidden_dims=(50,50,50), flatten: bool = False):\n",
    "        blocks = list()\n",
    "        input_dim_block = input_dim  # initially R^{ (p+1) * d }\n",
    "        for hidden_dim in hidden_dims:\n",
    "            blocks.append(ResidualBlock(input_dim_block, hidden_dim))  # layers A_1, R_2, and R_3\n",
    "            input_dim_block = hidden_dim  # becomes R^{ 50 }\n",
    "        blocks.append(nn.Linear(input_dim_block, output_dim))  # layer A_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3] Summarizing & Plotting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Call `create_summary()` from `./lib/plot.py`\n",
    "    1. Take the length-$(p+q)$ real paths from the **test set**. (Call the last $q$ entries `x_real_future`)\n",
    "    2. Use their first $p$ entries to generate the next $q$ fake entries. (Call the $q$ entries `x_fake_future`)\n",
    "    3. Call `plot_summary()` to compare `x_real_future` and `x_fake_future` by visualizing their distributions (histogram) and autocorrelation graph.\n",
    "2. Plot and pickle and the paths\n",
    "3. Plot the loss progressions. See `get_standard_test_metrics()` in `./lib/algos/base.py` for all the losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Input BTC & ETH 1 hour Binance close.\n",
    "* $p=24$, $q=6$\n",
    "* Algorithm: Conditional Weierstrass GAN (CWGAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](.\\numerical_results\\BINANCE\\BTC_ETH\\seed=0\\CWGAN\\summary.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_{ \\text{real future, test} }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](.\\numerical_results\\BINANCE\\BTC_ETH\\seed=0\\CWGAN\\x_real_test.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_{ \\text{fake future} }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](.\\numerical_results\\BINANCE\\BTC_ETH\\seed=0\\CWGAN\\x_fake_future.png \"Title\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
